<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>devops on // √Ålex Go{,5z}</title><link>https://agonzalezro.github.io/tags/devops/</link><description>Recent content in devops on // √Ålex Go{,5z}</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Mon, 16 Apr 2018 00:00:00 +0000</lastBuildDate><atom:link href="https://agonzalezro.github.io/tags/devops/index.xml" rel="self" type="application/rss+xml"/><item><title>How to prepare the Certified Kubernetes Administrator exam</title><link>https://agonzalezro.github.io/posts/cka/</link><pubDate>Mon, 16 Apr 2018 00:00:00 +0000</pubDate><guid>https://agonzalezro.github.io/posts/cka/</guid><description>
&lt;p>Finally last weekend I did the CKA exam, I was procrastinating it a bit in part because I didn&amp;rsquo;t know what to expect since Kubernetes is huge and I thought that each day extra I spend reading about it would help me.&lt;/p>
&lt;p>TLDR; I did pass the exam and if you work daily with Kubernetes or variants as Openshift you will be ok, they are not trying to catch you in some weird question that not even &lt;a href="https://twitter.com/kelseyhightower">Kelsey&lt;/a> would know how to answer üòÄ&lt;/p>
&lt;p>&lt;img src="https://agonzalezro.github.io/devops/cka.png" alt="CKA certification">&lt;/p>
&lt;h2 id="preparation">Preparation&lt;/h2>
&lt;p>Giving the long quantity of topics that could be covered in the exam I have used several resources to study it.
I will try to sort this in a way that makes sense if you have no knowledge about Kubernetes at all, if you are already familiarized with it you probably can skip some of the links.&lt;/p>
&lt;p>First of all you will need to know the basic Kubernetes resources, if you are a video fan maybe this video could help you: &lt;a href="https://www.youtube.com/embed/K_Kh4LMiiqQ">&amp;ldquo;From pets to cattle, the way of Kubernetes&amp;rdquo;&lt;/a> (no spam üò¨) but after watching it you will need to get more in depth, for doing so I recommend the book &lt;a href="http://shop.oreilly.com/product/0636920043874.do">&amp;ldquo;Kubernetes: Up and Running&amp;rdquo;&lt;/a>.&lt;/p>
&lt;p>Now that you know the basics probably you will want to test it, you have several ways for doing it: minikube, creating a cluster in GKE, etc&amp;hellip; but if you don&amp;rsquo;t want to bother about it yet, use &lt;a href="https://www.katacoda.com/">Katacoda&lt;/a> and try to finish some of the free Scenarios they have there.&lt;/p>
&lt;p>You are almost there but if you are missing troubleshooting experience because you never administrated a Kubernetes cluster take a look to: &lt;a href="http://shop.oreilly.com/product/0636920064947.do">&amp;ldquo;Kubernetes Cookbook&amp;rdquo;&lt;/a>; this book is pretty focused on pragmatic problems, it will definitely help you in case you need to troubleshoot stuff and try to follow &lt;a href="https://github.com/kelseyhightower/kubernetes-the-hard-way">&amp;ldquo;Kubernetes the hard way&amp;rdquo;&lt;/a> that would help you a lot to understand how to install or fix the Kubernetes components.&lt;/p>
&lt;p>If have read all the previous links but you still want more, take a look to the official documentation at &lt;a href="https://kubernetes.io/">kubernetes.io&lt;/a> to review all the concepts.&lt;/p>
&lt;h2 id="the-day-of-the-exam">The day of the exam&lt;/h2>
&lt;p>Part of my anxiety regarding the exam was that I didn&amp;rsquo;t know what to expect after sitting in my PC at 3pm, what would happen if my browser gets blocked? If I can&amp;rsquo;t understand a question? If I break my cluster? What&amp;rsquo;s going to be my &amp;ldquo;IDE&amp;rdquo;?
I think some of these questions could be easily answered by the Linux Foundation in form of a video explaining how to do the exam, but I couldn&amp;rsquo;t find anything similar so I will try to help you here:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>Before the exam, the examiner will ask you yo &lt;strong>clean your desk and drawers&lt;/strong>, completely, I had some books in drawers that were not covered by the cam and I had to move them. I also had an schedule in a wall and I had to unglue it.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>The &lt;strong>place should be quiet&lt;/strong> because even if you usually work with headphones you will not be allowed to use them.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>The examiner will ask you to see all the room, even under the desk.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>The &lt;strong>examiner will not talk to you by voice, only by chat&lt;/strong>. He/she will hear you because you will need to share the screen and micro. When you do that he will ask you to show open processes et al to be sure you are not copying. I found this a little bit excessive, but&amp;hellip; rules and probably some previous cheaters (shame on you!).&lt;/p>
&lt;/li>
&lt;li>
&lt;p>The exam happens in a Chrome tab, the left side will show you the questions and the percentage it covers and you will be able to jump from one to another. All the questions are atomic. The right side is for the shell, I tried to use tmux there, but it was pretty difficult inside a browser terminal. You can also have a popup with notes.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>After running a command in the shell it got blocked and I panicked, I went to my Inbox to write an email to support but luckily after few second (maybe a minute or two but it looked like a long time to me) it went back. I let the examiner know just in case he/she saw me in the email, and that&amp;rsquo;s it.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>You can &lt;strong>only open a tab with kubernetes.io&lt;/strong> and use its search box, no Google. In the past it seems it wasn&amp;rsquo;t like this. It&amp;rsquo;s ok because the answer is usually there, but you need to get used to it, maybe try it out during the preparation.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>It&amp;rsquo;s ok to request a pause&lt;/strong>, but careful because the time doesn&amp;rsquo;t stop.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>You have three hours to finish the exam, &lt;strong>if you get blocked it‚Äôs better to skip&lt;/strong> that question for now and retake it later.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h2 id="the-wait">The wait&lt;/h2>
&lt;p>I did the exam last Saturday afternoon, from 3pm to 6pm and when I work up this morning I already had the result in my Inbox so you will not need to wait a lot for it, probably during work days it&amp;rsquo;s going to be even quicker.&lt;/p>
&lt;p>&lt;img src="https://agonzalezro.github.io/devops/thatsall.jpg" alt="that&amp;rsquo;s all folks">&lt;/p>
&lt;p>So, that&amp;rsquo;s it from me.
If you are interested in Kubernetes and you work with it often you will not have any trouble passing the exam, however, I understand that giving the huge range of things that they could ask you the exam can be intimidating, trust yourself and you will be ok!
If you do the exam and this helps you somehow or you just want to let me know anything, &lt;a href="https://twitter.com/agonzalezro">please ping me on @agonzalezro&lt;/a>!&lt;/p></description></item><item><title>Scheduling Your Kubernetes Pods With Elixir</title><link>https://agonzalezro.github.io/posts/elixir-scheduler/</link><pubDate>Mon, 16 Jan 2017 00:00:00 +0000</pubDate><guid>https://agonzalezro.github.io/posts/elixir-scheduler/</guid><description>
&lt;blockquote>
&lt;p>This was originally posted on &lt;a href="https://deis.com/blog/2016/scheduling-your-kubernetes-pods-with-elixir/">deis.com&lt;/a>.&lt;/p>
&lt;/blockquote>
&lt;p>&lt;a href="https://twitter.com/kelseyhightower">Kelsey Hightower&lt;/a> gave a really interesting talk at &lt;a href="https://skillsmatter.com/conferences/7208-containersched-2015">ContainerSched&lt;/a> about how to create your own scheduler using the Kubernetes HTTP API.&lt;/p>
&lt;p>The talk was awesome. It&amp;rsquo;s incredible to see what kind of things you can do with a base system as good as Kubernetes.&lt;/p>
&lt;p>However, I missed one thing. The &lt;a href="https://github.com/kelseyhightower/scheduler">example&lt;/a> provided by Kelsey was a Go application. Which is the main language used with Kubernetes. So, if you look at that code without any context, you might think it&amp;rsquo;s using some kind of Kubernetes internal packages. But it&amp;rsquo;s not! It&amp;rsquo;s a standalone piece of code that happens to make some HTTP calls.&lt;/p>
&lt;p>To illustrate this point, I decided to write my own scheduler, in a different language. In my case, &lt;a href="http://elixir-lang.org/">Elixir&lt;/a>, because that&amp;rsquo;s the language I happen to be learning at the moment.&lt;/p>
&lt;p>This post isn&amp;rsquo;t an intro to Elixir, but the code should be easy to follow.&lt;/p>
&lt;p>Also, I&amp;rsquo;m going to use &lt;code>localhost&lt;/code> when accessing the Kubernetes API. Why? For simplicity. If we run &lt;code>kubectl proxy&lt;/code> on a computer connected to the Kubernetes master, we will not need to deal with authorization, hosts, and so on. The &lt;code>proxy&lt;/code> command will do it for us.&lt;/p>
&lt;p>So, let&amp;rsquo;s dive in.&lt;/p>
&lt;h2 id="get-a-list-of-unscheduled-jobs">Get a List of Unscheduled Jobs&lt;/h2>
&lt;p>To start off with, we need a pod for our scheduler to schedule.&lt;/p>
&lt;p>By default, Kubernetes will schedule all your pods. So, we need to create a unscheduled pod and we need to indicate to Kubernetes that we don&amp;rsquo;t want our pod to get scheduled.&lt;/p>
&lt;p>So, create a file called &lt;code>pod.yml&lt;/code> and put this inside:&lt;/p>
&lt;div class="highlight">&lt;pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="color:#f92672">apiVersion&lt;/span>: &lt;span style="color:#ae81ff">v1&lt;/span>
&lt;span style="color:#f92672">kind&lt;/span>: &lt;span style="color:#ae81ff">Pod&lt;/span>
&lt;span style="color:#f92672">metadata&lt;/span>:
&lt;span style="color:#f92672">name&lt;/span>: &lt;span style="color:#ae81ff">custom&lt;/span>
&lt;span style="color:#f92672">annotations&lt;/span>:
&lt;span style="color:#f92672">scheduler.alpha.kubernetes.io/name&lt;/span>: &lt;span style="color:#ae81ff">myOwnScheduler&lt;/span>
&lt;span style="color:#f92672">spec&lt;/span>:
&lt;span style="color:#f92672">containers&lt;/span>:
- &lt;span style="color:#f92672">name&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;nginx&amp;#34;&lt;/span>
&lt;span style="color:#f92672">image&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;nginx:1.10.0&amp;#34;&lt;/span>
&lt;/code>&lt;/pre>&lt;/div>&lt;p>Notice the &lt;code>scheduler.alpha.kubernetes.io/name&lt;/code> annotation we use to specify that our scheduler (that we call &lt;code>myOwnScheduler&lt;/code>) will be handling this pod.&lt;/p>
&lt;p>Now, let&amp;rsquo;s create the pod from the definition file:&lt;/p>
&lt;div class="highlight">&lt;pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-bash" data-lang="bash">kubectl create -f pod.yml
&lt;/code>&lt;/pre>&lt;/div>&lt;p>After you&amp;rsquo;ve done this, there will be an unscheduled job waiting for us inside Kubernetes. Why unscheduled? Because our pod says it wants to be scheduled by the &lt;code>myOwnScheduler&lt;/code> scheduler, but we&amp;rsquo;ve not created it yet, so Kubernetes can&amp;rsquo;t schedule it.&lt;/p>
&lt;p>So, let&amp;rsquo;s build our scheduler.&lt;/p>
&lt;p>First, our scheduler must be able to grab this unscheduled job from the API.&lt;/p>
&lt;p>Here&amp;rsquo;s the function I wrote to do that:&lt;/p>
&lt;div class="highlight">&lt;pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-elixir" data-lang="elixir">&lt;span style="color:#66d9ef">def&lt;/span> unscheduled_pods &lt;span style="color:#66d9ef">do&lt;/span>
is_managed_by_us &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#f92672">&amp;amp;&lt;/span>(get_in(&amp;amp;1, [&lt;span style="color:#e6db74">&amp;#34;metadata&amp;#34;&lt;/span>, &lt;span style="color:#e6db74">&amp;#34;annotations&amp;#34;&lt;/span>, &lt;span style="color:#e6db74">&amp;#34;scheduler.alpha.kubernetes.io/name&amp;#34;&lt;/span>]) &lt;span style="color:#f92672">==&lt;/span> &lt;span style="color:#a6e22e">@name&lt;/span>)
resp &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">HTTPoison&lt;/span>&lt;span style="color:#f92672">.&lt;/span>get! &lt;span style="color:#e6db74">&amp;#34;http://127.0.0.1:8001/api/v1/pods?fieldSelector=spec.nodeName=&amp;#34;&lt;/span>
resp&lt;span style="color:#f92672">.&lt;/span>body
&lt;span style="color:#f92672">|&amp;gt;&lt;/span> &lt;span style="color:#a6e22e">Poison&lt;/span>&lt;span style="color:#f92672">.&lt;/span>decode!
&lt;span style="color:#f92672">|&amp;gt;&lt;/span> get_in([&lt;span style="color:#e6db74">&amp;#34;items&amp;#34;&lt;/span>])
&lt;span style="color:#f92672">|&amp;gt;&lt;/span> &lt;span style="color:#a6e22e">Enum&lt;/span>&lt;span style="color:#f92672">.&lt;/span>filter(is_managed_by_us)
&lt;span style="color:#f92672">|&amp;gt;&lt;/span> &lt;span style="color:#a6e22e">Enum&lt;/span>&lt;span style="color:#f92672">.&lt;/span>map(&lt;span style="color:#f92672">&amp;amp;&lt;/span>(get_in(&amp;amp;1, [&lt;span style="color:#e6db74">&amp;#34;metadata&amp;#34;&lt;/span>, &lt;span style="color:#e6db74">&amp;#34;name&amp;#34;&lt;/span>])))
&lt;span style="color:#66d9ef">end&lt;/span>
&lt;/code>&lt;/pre>&lt;/div>&lt;p>As you can see we are using &lt;code>127.0.0.1&lt;/code> to query our API, this is posible thanks to the &lt;code>kubectl proxy&lt;/code> command we mentioned in the intro.&lt;/p>
&lt;p>This code grabs the name of all the pods that are waiting to be scheduled inside Kubernetes. It then uses the &lt;code>is_managed_by_us&lt;/code> function to see whether the &lt;code>@name&lt;/code> attribute is set to &lt;code>myOwnScheduler&lt;/code>. If this is true, the pod has indicated that it should be managed by our scheduler.&lt;/p>
&lt;h2 id="an-elixir-introduction">An Elixir Introduction&lt;/h2>
&lt;p>I said I wouldn&amp;rsquo;t explain Elixir, but I will just comment on three piece of syntax.&lt;/p>
&lt;p>The &lt;code>|&amp;gt;&lt;/code> in the code above is a pipe, like the &lt;code>|&lt;/code> character from a shell script. Using this passes the result from the left side as a parameter to the function on the right side. In this way return values can be piped through functions.&lt;/p>
&lt;p>The &lt;code>&amp;amp;&lt;/code> character marks an &lt;a href="https://en.wikipedia.org/wiki/Anonymous_function">anonymous function&lt;/a>, i.e. a function that we define inline as an expression. And &lt;code>&amp;amp;1&lt;/code> refers the first parameter received by the function.&lt;/p>
&lt;p>We can also use an anonymous function as parameter to another function. As we are doing, in the &lt;code>map&lt;/code> call.&lt;/p>
&lt;h2 id="get-a-list-of-available-nodes">Get a List of Available Nodes&lt;/h2>
&lt;p>Our &lt;code>unscheduled_pods&lt;/code> function gets us a list of pods that have not been scheduled yet. So next up, we can bind them to our nodes. That is, we tell our containers to run on particular nodes.&lt;/p>
&lt;p>But wait, how do we know what nodes are available?&lt;/p>
&lt;p>We&amp;rsquo;ll need to grab those as well, like this:&lt;/p>
&lt;div class="highlight">&lt;pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-elixir" data-lang="elixir">&lt;span style="color:#66d9ef">def&lt;/span> nodes &lt;span style="color:#66d9ef">do&lt;/span>
resp &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">HTTPoison&lt;/span>&lt;span style="color:#f92672">.&lt;/span>get! &lt;span style="color:#e6db74">&amp;#34;http://127.0.0.1:8001/api/v1/nodes&amp;#34;&lt;/span>
resp&lt;span style="color:#f92672">.&lt;/span>body
&lt;span style="color:#f92672">|&amp;gt;&lt;/span> &lt;span style="color:#a6e22e">Poison&lt;/span>&lt;span style="color:#f92672">.&lt;/span>decode!
&lt;span style="color:#f92672">|&amp;gt;&lt;/span> get_in([&lt;span style="color:#e6db74">&amp;#34;items&amp;#34;&lt;/span>])
&lt;span style="color:#f92672">|&amp;gt;&lt;/span> &lt;span style="color:#a6e22e">Enum&lt;/span>&lt;span style="color:#f92672">.&lt;/span>map(&lt;span style="color:#f92672">&amp;amp;&lt;/span>(get_in(&amp;amp;1, [&lt;span style="color:#e6db74">&amp;#34;metadata&amp;#34;&lt;/span>, &lt;span style="color:#e6db74">&amp;#34;name&amp;#34;&lt;/span>])))
&lt;span style="color:#66d9ef">end&lt;/span>
&lt;/code>&lt;/pre>&lt;/div>&lt;p>This code will return a list of all nodes.&lt;/p>
&lt;p>We could potentially request a lot more information from the API at this point, but for what we&amp;rsquo;re building, it&amp;rsquo;s not necessary. If you want to now what extra information is available, &lt;a href="http://kubernetes.io/kubernetes/third_party/swagger-ui/#!/api%2Fv1/listNamespacedNode">check the documentation for this endpoint&lt;/a>.&lt;/p>
&lt;h2 id="the-bind-function">The Bind Function&lt;/h2>
&lt;p>So let&amp;rsquo;s review.&lt;/p>
&lt;p>We have a list of all the unscheduled jobs that we are supposed to manage. And we have a list of all the nodes we can schedule jobs to.&lt;/p>
&lt;p>But how do we actually schedule a pods on a node?&lt;/p>
&lt;p>We call the bind endpoint, like this:&lt;/p>
&lt;div class="highlight">&lt;pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-elixir" data-lang="elixir">&lt;span style="color:#66d9ef">def&lt;/span> bind(pod_name, node_name) &lt;span style="color:#66d9ef">do&lt;/span>
url &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#e6db74">&amp;#34;http://127.0.0.1:8001/api/v1/namespaces/default/pods/&lt;/span>&lt;span style="color:#e6db74">#{&lt;/span>pod&lt;span style="color:#e6db74">}&lt;/span>&lt;span style="color:#e6db74">/binding&amp;#34;&lt;/span>
payload &lt;span style="color:#f92672">=&lt;/span> %{
&lt;span style="color:#e6db74">&amp;#34;apiVersion&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;v1&amp;#34;&lt;/span>,
&lt;span style="color:#e6db74">&amp;#34;kind&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;Binding&amp;#34;&lt;/span>,
&lt;span style="color:#e6db74">&amp;#34;metadata&amp;#34;&lt;/span>: %{
&lt;span style="color:#e6db74">&amp;#34;name&amp;#34;&lt;/span>: pod_name,
},
&lt;span style="color:#e6db74">&amp;#34;target&amp;#34;&lt;/span>: %{
&lt;span style="color:#e6db74">&amp;#34;apiVersion&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;v1&amp;#34;&lt;/span>,
&lt;span style="color:#e6db74">&amp;#34;kind&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;Node&amp;#34;&lt;/span>,
&lt;span style="color:#e6db74">&amp;#34;name&amp;#34;&lt;/span>: node_name,
}
}
headers &lt;span style="color:#f92672">=&lt;/span> [{&lt;span style="color:#e6db74">&amp;#39;content-type&amp;#39;&lt;/span>, &lt;span style="color:#e6db74">&amp;#39;application/json&amp;#39;&lt;/span>}]
&lt;span style="color:#a6e22e">HTTPoison&lt;/span>&lt;span style="color:#f92672">.&lt;/span>post! url, payload &lt;span style="color:#f92672">|&amp;gt;&lt;/span> &lt;span style="color:#a6e22e">Poison&lt;/span>&lt;span style="color:#f92672">.&lt;/span>encode!, headers
&lt;span style="color:#a6e22e">IO&lt;/span>&lt;span style="color:#f92672">.&lt;/span>puts &lt;span style="color:#e6db74">&amp;#34;&lt;/span>&lt;span style="color:#e6db74">#{&lt;/span>pod_name&lt;span style="color:#e6db74">}&lt;/span>&lt;span style="color:#e6db74"> pod scheduled in &lt;/span>&lt;span style="color:#e6db74">#{&lt;/span>node_name&lt;span style="color:#e6db74">}&lt;/span>&lt;span style="color:#e6db74">&amp;#34;&lt;/span>
&lt;span style="color:#66d9ef">end&lt;/span>
&lt;/code>&lt;/pre>&lt;/div>&lt;p>But before we can use this function, we need a scheduling strategy.&lt;/p>
&lt;h2 id="scheduling-strategy">Scheduling Strategy&lt;/h2>
&lt;p>For our simple scheduler, we will go with a random scheduling strategy.&lt;/p>
&lt;p>In effect, we schedule each pod on a random node. Multiple pods might even end up on the same node.&lt;/p>
&lt;p>Here&amp;rsquo;s how we do that:&lt;/p>
&lt;div class="highlight">&lt;pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-elixir" data-lang="elixir">&lt;span style="color:#66d9ef">def&lt;/span> schedule(pods) &lt;span style="color:#66d9ef">do&lt;/span>
pods
&lt;span style="color:#f92672">|&amp;gt;&lt;/span> &lt;span style="color:#a6e22e">Enum&lt;/span>&lt;span style="color:#f92672">.&lt;/span>each(&lt;span style="color:#f92672">&amp;amp;&lt;/span>(bind(&amp;amp;1, &lt;span style="color:#a6e22e">Enum&lt;/span>&lt;span style="color:#f92672">.&lt;/span>random(nodes))))
&lt;span style="color:#66d9ef">end&lt;/span>
&lt;/code>&lt;/pre>&lt;/div>&lt;p>Of course, this is very basic, and there are much better ways to maximise the resources we have available to us. But we&amp;rsquo;re building this as a learning exercise. Feel free to extend this code if you want to take things further.&lt;/p>
&lt;p>Some ideas to get you going:&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://en.wikipedia.org/wiki/Round-robin_scheduling">Round-robin&lt;/a> pod scheduling.&lt;/li>
&lt;li>Implement your own version of Kubernete&amp;rsquo;s load-aware scheduling.&lt;/li>
&lt;li>Annotate the pods with some extra information, for example: cluster.type: raspberry if you want to send them to the Raspberry Pi nodes in your cluster.&lt;/li>
&lt;li>You could use some external source of truth, e.g. Nagios, to determine which nodes to schedule jobs on.&lt;/li>
&lt;/ul>
&lt;h2 id="putting-everything-together">Putting Everything Together&lt;/h2>
&lt;p>Now we have everything in place, a &lt;code>main&lt;/code> function is all that&amp;rsquo;s needed:&lt;/p>
&lt;div class="highlight">&lt;pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-elixir" data-lang="elixir">&lt;span style="color:#66d9ef">def&lt;/span> main(_args) &lt;span style="color:#66d9ef">do&lt;/span>
unscheduled_pods
&lt;span style="color:#f92672">|&amp;gt;&lt;/span> schedule
&lt;span style="color:#66d9ef">end&lt;/span>
&lt;/code>&lt;/pre>&lt;/div>&lt;p>Now you are able to run your program!&lt;/p>
&lt;p>A simple &lt;code>elixir your_script.exs&lt;/code> should do it, if you have all the dependencies. But you probably don&amp;rsquo;t, so I recommend you follow the Usage section in &lt;a href="https://github.com/agonzalezro/escheduler#usage">the README&lt;/a> for the code that accompanies this post.&lt;/p>
&lt;h2 id="wrap-up">Wrap Up&lt;/h2>
&lt;p>For a most things, you probably don&amp;rsquo;t want to write your own scheduler. The one provided by Kubernetes is good for so many things.&lt;/p>
&lt;p>But if you want to do something the default Kubernetes scheduler can&amp;rsquo;t do, it&amp;rsquo;s not as difficult as you might think to write your own and slot it in, and continue to take advantage of everything Kubernetes has to offer.&lt;/p>
&lt;p>I&amp;rsquo;ve put my code up for &lt;a href="https://github.com/agonzalezro/escheduler">my Elixir scheduler&lt;/a>, on GitHub in case you want to check it out. This repository has everything you need to follow along with this post, from the YAML pod definition to the scheduler itself.&lt;/p></description></item><item><title>Signing your GitHub work with your Keybase keys</title><link>https://agonzalezro.github.io/posts/keybase/</link><pubDate>Wed, 06 Apr 2016 00:00:00 +0000</pubDate><guid>https://agonzalezro.github.io/posts/keybase/</guid><description>
&lt;p>As you may know, yesterday, &lt;a href="https://github.com/blog/2144-gpg-signature-verification">GitHub has presented a way to verify your commits&lt;/a> on their platform. This is not something new, it&amp;rsquo;s part of the git protocol, but now, they will show a fancy &amp;ldquo;verified&amp;rdquo; label everytime you push a signed commit. Cool? Yeah, pretty cool.&lt;/p>
&lt;p>One of the problems I always had since I know GPG is to keep my keys locally. I am pretty sure that some other people take care of their keys as if they were part of their family, but I didn&amp;rsquo;t feel that need yet. &lt;a href="https://keybase.io/">Keybase&lt;/a> partially solved this problem for me, let&amp;rsquo;s hope it did it for you as well (ask me for an invite if you don&amp;rsquo;t have one yet).&lt;/p>
&lt;p>&lt;em>I want to start this article with a disclaimer, I am not a security professional or anything even closer to it; so, if you think I am saying anything stupid, please, do let me know. Also, if you are a heavy user of Keybase, take my steps carefully because I don&amp;rsquo;t know if you account will continue being usable by Keybase (I would expect so, but a disclaimer is always good).&lt;/em>&lt;/p>
&lt;p>&lt;strong>What do we need to do then?&lt;/strong> Imagine that you are myself and you don&amp;rsquo;t have &lt;code>keybase&lt;/code> installed yet, also, you have a Mac:&lt;/p>
&lt;pre>&lt;code>brew install keybase
&lt;/code>&lt;/pre>
&lt;p>Now go to your Keybase account and download the public and private key, if you don&amp;rsquo;t know how to do it, follow &lt;a href="http://www.keybits.net/2016/02/import-keybase-private-key/">this tutorial by Tom Atkins&lt;/a>. Summarising it:&lt;/p>
&lt;ol>
&lt;li>Go to your Keybase account.&lt;/li>
&lt;li>Click on your key fingerprint and copy/paste that on a file.&lt;/li>
&lt;li>Click on &amp;ldquo;edit&amp;rdquo;, then &amp;ldquo;export my private key&amp;rdquo; and copy/paste that as well in a different file.&lt;/li>
&lt;li>Now, &lt;code>gpg --allow-secret-key-import --import keybase-private.key&lt;/code>.&lt;/li>
&lt;li>And, &lt;code>gpg --import keybase-public.key&lt;/code>.&lt;/li>
&lt;/ol>
&lt;p>You are good to go now.&lt;/p>
&lt;p>The problem that you will face trying to use your Keybase account is that your email address will look something like &lt;code>username@keybase.io&lt;/code> and you can not verify that account on github. For that reasonwe will need to update our key for adding your normal account email and delete the keybase one.&lt;/p>
&lt;p>So, let&amp;rsquo;s add your normal account:&lt;/p>
&lt;pre>&lt;code># alex @ Alexs-MacBook-Pro in ~/Desktop [18:14:34]
$ gpg --edit-key agonzalezro@keybase.io
...
gpg&amp;gt; adduid
Real name: Alexandre Gonz√°lez Rodr√≠guez
Email address: agonzalezro@gmail.com
Comment:
You are using the `utf-8' character set.
You selected this USER-ID:
&amp;quot;Alexandre Gonz√°lez Rodr√≠guez &amp;lt;agonzalezro@gmail.com&amp;gt;&amp;quot;
Change (N)ame, (C)omment, (E)mail or (O)kay/(Q)uit? o
You need a passphrase to unlock the secret key for
user: &amp;quot;keybase.io/agonzalezro &amp;lt;agonzalezro@keybase.io&amp;gt;&amp;quot;
4096-bit RSA key, ID 68C84A97, created 2014-03-11
pub 4096R/68C84A97 created: 2014-03-11 expires: 2024-03-08 usage: SCEA
trust: unknown validity: unknown
sub 4096R/74F6172C created: 2014-03-11 expires: 2024-03-08 usage: SEA
[ unknown] (1) keybase.io/agonzalezro &amp;lt;agonzalezro@keybase.io&amp;gt;
[ unknown] (2). Alexandre Gonz√°lez Rodr√≠guez &amp;lt;agonzalezro@gmail.com&amp;gt;
&lt;/code>&lt;/pre>&lt;p>&lt;strong>Update: Chris Miller showed me on the comments that this step is not really needed, so don&amp;rsquo;t delete you key, just write &lt;code>q&lt;/code> to exit and save the changes.&lt;/strong>&lt;/p>
&lt;p>And delete the &lt;code>keybase.io&lt;/code> one:&lt;/p>
&lt;pre>&lt;code>...
gpg&amp;gt; 1
pub 4096R/68C84A97 created: 2014-03-11 expires: 2024-03-08 usage: SCEA
trust: unknown validity: unknown
sub 4096R/74F6172C created: 2014-03-11 expires: 2024-03-08 usage: SEA
[ unknown] (1). Alexandre Gonz√°lez Rodr√≠guez &amp;lt;agonzalezro@gmail.com&amp;gt;
[ unknown] (2)* keybase.io/agonzalezro &amp;lt;agonzalezro@keybase.io&amp;gt;
gpg&amp;gt; deluid
Really remove this user ID? (y/N) y
pub 4096R/68C84A97 created: 2014-03-11 expires: 2024-03-08 usage: SCEA
trust: unknown validity: unknown
sub 4096R/74F6172C created: 2014-03-11 expires: 2024-03-08 usage: SEA
[ unknown] (1). Alexandre Gonz√°lez Rodr√≠guez &amp;lt;agonzalezro@gmail.com&amp;gt;
gpg&amp;gt; q
Save changes? (y/N) y
&lt;/code>&lt;/pre>&lt;p>What we did there on the first line was select the account &lt;code>keybase.io&lt;/code>, please, check the number associated with your &lt;code>keybase.io&lt;/code> account, don&amp;rsquo;t press &lt;code>1&lt;/code> just for the sake of it.&lt;/p>
&lt;p>You have everything you need now, so you just need to export your key and &lt;a href="https://github.com/settings/keys">import it on github&lt;/a>:&lt;/p>
&lt;pre>&lt;code>gpg --armor --export agonzalezro@gmail.com
&lt;/code>&lt;/pre>&lt;p>PS: &lt;code>pbcopy&lt;/code> is your friend here if you are using Mac :)&lt;/p>
&lt;p>The only thing you need to do now is sign a commit, do that with &lt;code>git commit -S&lt;/code> and after pushing you will have this beautiful &amp;ldquo;label&amp;rdquo; close to your commit.&lt;/p>
&lt;p>&lt;a href="https://github.com/agonzalezro/agonzalezro.github.io/commits/polo">&lt;img src="https://agonzalezro.github.io/github_verified_commit.png" alt="github verified commit">&lt;/a>&lt;/p>
&lt;p>I hope that you enjoyed the tutorial, let me know on the comments or on twitter (&lt;a href="https://twitter.com/agonzalezro">@agonzalezro&lt;/a>) if something I do is stupid or if you have found it useful!&lt;/p></description></item><item><title>Using Kong with Kubernetes</title><link>https://agonzalezro.github.io/posts/kong-k8s/</link><pubDate>Thu, 17 Dec 2015 00:00:00 +0000</pubDate><guid>https://agonzalezro.github.io/posts/kong-k8s/</guid><description>
&lt;blockquote>
&lt;p>This was originally posted on &lt;a href="http://k8s.uk/using-kong-with-kubernetes.html">k8s.uk&lt;/a>.&lt;/p>
&lt;/blockquote>
&lt;p>If you don&amp;rsquo;t know about &lt;a href="https://getkong.org">Kong&lt;/a> yet, you should take a look. It&amp;rsquo;s an Open Source API Gateway, they define themselves as: &amp;ldquo;The open-source management layer for APIs, delivering high performance and reliability.&amp;rdquo; and they are quite right.&lt;/p>
&lt;p>I was playing with Kong lately at work (&lt;a href="http://jobandtalent.com">jobandtalent.com&lt;/a>, we are hiring!) and I think that it could be pretty awesome as a entry layer to your microservices platform running in Kubernetes.&lt;/p>
&lt;p>For the sake of simplicity I will not run Kong in Kubernetes, but it shouldn&amp;rsquo;t be so difficult since &lt;a href="https://getkong.org/install/">they already provide Docker images&lt;/a>. Also, running Kong on the same cluster you will be able to use internal networking between pods: win-win.&lt;/p>
&lt;p>So, what will I show?&lt;/p>
&lt;ul>
&lt;li>I will deploy a Kubernetes with 2 pods (our 2 microservices) &amp;amp;&lt;/li>
&lt;li>I will install Kong locally and configure it to point to this 2 services.&lt;/li>
&lt;/ul>
&lt;h2 id="go--packing">Go &amp;amp; packing&lt;/h2>
&lt;p>I&amp;rsquo;ve created a small Go app that will show the value of an environment variable when you &lt;code>GET /&lt;/code>:&lt;/p>
&lt;pre>&lt;code>package main
import (
&amp;quot;fmt&amp;quot;
&amp;quot;log&amp;quot;
&amp;quot;net/http&amp;quot;
&amp;quot;os&amp;quot;
)
func main() {
http.HandleFunc(&amp;quot;/&amp;quot;, func(w http.ResponseWriter, r *http.Request) {
fmt.Fprintf(w, os.Getenv(&amp;quot;TEST_RESULT&amp;quot;))
})
log.Fatal(http.ListenAndServe(&amp;quot;:8080&amp;quot;, nil))
}
&lt;/code>&lt;/pre>
&lt;p>Now we will build the application to later pack it in our image. Remember that if you are in Mac you will need to cross-compile the app to work on Linux:&lt;/p>
&lt;pre>&lt;code>$ GOOS=linux go build
&lt;/code>&lt;/pre>
&lt;p>We can pack it into our images now. For doing so we need a &lt;code>Dockerfile&lt;/code>. It&amp;rsquo;s a simple binary, so the &lt;code>Dockerfile&lt;/code> is not complex at all:&lt;/p>
&lt;pre>&lt;code>FROM scratch
ADD app /
ENTRYPOINT [&amp;quot;/app&amp;quot;]
&lt;/code>&lt;/pre>
&lt;p>Cool! What can we do now with our shiny image? Yes, you are right! Push it to the hub:&lt;/p>
&lt;pre>&lt;code>$ docker build -t agonzalezro/kong-test .
$ docker push agonzalezro/kong-test
&lt;/code>&lt;/pre>
&lt;h2 id="k8s">k8s&lt;/h2>
&lt;p>We have our image on the registry and all we need now is running it on Kubernetes. I am using &lt;a href="https://cloud.google.com/container-engine/">Google Container Engine&lt;/a> for deploying this, but you can use whatever you prefer.&lt;/p>
&lt;p>Let&amp;rsquo;s create our RCs &amp;amp; services:&lt;/p>
&lt;pre>&lt;code># rc1.yml
apiVersion: v1
kind: ReplicationController
metadata:
name: api1
spec:
selector:
name: api
version: first
template:
metadata:
labels:
name: api
version: first
spec:
containers:
- name: app
image: agonzalezro/kong-test
env:
- name: TEST_RESULT
value: &amp;quot;This is the first app&amp;quot;
# rc2.yml
apiVersion: v1
kind: ReplicationController
metadata:
name: api2
spec:
selector:
name: api
version: second
template:
metadata:
labels:
name: api
version: second
spec:
containers:
- name: app
image: agonzalezro/kong-test
env:
- name: TEST_RESULT
value: &amp;quot;Second!&amp;quot;
# svc1.yml
apiVersion: v1
kind: Service
metadata:
name: app1-svc
spec:
type: LoadBalancer
ports:
- port: 80
targetPort: 8080
selector:
name: api
version: first
# svc2.yml
apiVersion: v1
kind: Service
metadata:
name: app2-svc
spec:
type: LoadBalancer
ports:
- port: 80
targetPort: 8080
selector:
name: api
version: second
&lt;/code>&lt;/pre>
&lt;p>And now run it:&lt;/p>
&lt;pre>&lt;code>$ kubectl create -f rc1.yml -f rc2.yml -f svc1.yml -f svc2.yml
&lt;/code>&lt;/pre>
&lt;p>Wait for the service and the pods to be ready and check their IPS:&lt;/p>
&lt;pre>&lt;code>$ kubectl get services
NAME CLUSTER_IP EXTERNAL_IP PORT(S) SELECTOR AGE
app1-svc 10.159.242.86 130.211.89.175 80/TCP name=api,version=first 17m
app2-svc 10.159.246.93 104.155.53.175 80/TCP name=api,version=second 17m
kubernetes 10.159.240.1 &amp;lt;none&amp;gt; 443/TCP &amp;lt;none&amp;gt; 1h
&lt;/code>&lt;/pre>
&lt;h2 id="kong">Kong&lt;/h2>
&lt;p>Follow the instruction here: &lt;a href="https://getkong.org/install/docker/">https://getkong.org/install/docker/&lt;/a> to install Kong locally.&lt;/p>
&lt;p>Yeah! We have it up &amp;amp; running so let&amp;rsquo;s point it to our shinny cluster. We need to use Kong API for that (port &lt;code>:8001&lt;/code>):&lt;/p>
&lt;pre>&lt;code>$ http http://dockerhost:8001/apis/ name=first upstream_url=http://130.211.89.175 request_path=/first strip_request_path=true
$ http http://dockerhost:8001/apis/ name=second upstream_url=http://104.155.53.175 request_path=/second strip_request_path=true
&lt;/code>&lt;/pre>
&lt;p>What we did here? We set up two new endpoints &lt;code>/first&lt;/code> &amp;amp; &lt;code>/second&lt;/code> that are pointing to the both Kubernetes services previously created. We could have done it with DNS as well using &lt;code>request_host&lt;/code> instead.&lt;/p>
&lt;p>Lets call Kong on the port &lt;code>:8000&lt;/code> to use them:&lt;/p>
&lt;pre>&lt;code>$ http http://dockerhost:8000/first
HTTP/1.1 200 OK
Connection: keep-alive
Content-Length: 21
Content-Type: text/plain; charset=utf-8
Date: Thu, 17 Dec 2015 21:43:41 GMT
Via: kong/0.5.4
This is the first app
$ http http://dockerhost:8000/second
HTTP/1.1 200 OK
Connection: keep-alive
Content-Length: 7
Content-Type: text/plain; charset=utf-8
Date: Thu, 17 Dec 2015 21:43:44 GMT
Via: kong/0.5.4
Second!
&lt;/code>&lt;/pre>
&lt;p>\o/ We did it!&lt;/p>
&lt;h2 id="next-steps">Next steps&lt;/h2>
&lt;p>You have Kong pointed to your cluster, now it&amp;rsquo;s up to your imagination what to do next. I would say try to configure some rate limiting or auth, it&amp;rsquo;s deadly simply. Check them here: &lt;a href="https://getkong.org/plugins/">https://getkong.org/plugins/&lt;/a>&lt;/p>
&lt;p>If you have any question or you want to discuss this further let me know at &lt;a href="https://twitter.com/agonzalezro">@agonzalezro&lt;/a>.&lt;/p></description></item><item><title>Log your Docker containers from a container with packetbeat</title><link>https://agonzalezro.github.io/posts/packetbeat-docker/</link><pubDate>Mon, 17 Aug 2015 00:00:00 +0000</pubDate><guid>https://agonzalezro.github.io/posts/packetbeat-docker/</guid><description>
&lt;p>&lt;a href="https://github.com/elastic/packetbeat">packetbeat&lt;/a> is one of the best
application performance management tools in the opensource community. It&amp;rsquo;s now
part of the &lt;a href="https://www.elastic.co/products/beats">beats&lt;/a> that Elastic provide
us to monitor our applications. What it does is analyze our data packets to
send some useful information to ElasticSearch.&lt;/p>
&lt;p>After the introduction you probably realize that we will need: ElasticSearch
installed and if you want to see something useful you will need Kibana as well.
There is a &lt;a href="https://www.elastic.co/guide/en/beats/packetbeat/current/packetbeat-getting-started.html">guide for getting
started&lt;/a>
that explains the process pretty clearly. The &amp;ldquo;problem&amp;rdquo; with that guide is that
you need to install everything in your machine, this blog post is a 101 to have
it quickly running into containers.&lt;/p>
&lt;p>As mentioned, we will need ES and Kibana:&lt;/p>
&lt;pre>&lt;code>docker run -d -p 9200:9200 -p 9300:9300 --name elasticsearch-pb elasticsearch
docker run -d -p 5601:5601 --name kibana-pb --link elasticsearch-pb:elasticsearch \
-e ELASTICSEARCH_URL=http://elasticsearch:9200 kibana
&lt;/code>&lt;/pre>
&lt;p>Now we have all the required things to run our packetbeat. There are few points
that I want to raise before doing so:&lt;/p>
&lt;ol>
&lt;li>packetbeat will be monitoring our network traffic, this means that our
container running packetbeat needs to have access to the host network.&lt;/li>
&lt;li>the last &lt;code>Dockerfile&lt;/code> I found regarding this was:
&lt;a href="https://github.com/packetbeat/packetbeat-docker">https://github.com/packetbeat/packetbeat-docker&lt;/a> by
&lt;a href="https://twitter.com/tudor_g">Tudor&lt;/a> (one of the original developers) but
it is a little bit old. So I made my own based on that (should PR but I
wonder why the previous was not updated yet, and it would be nice to run
something more actual than 0.5 but there are no binaries):&lt;/li>
&lt;/ol>
&lt;!-- raw HTML omitted -->
&lt;p>The previous docker image will require a file &lt;code>packetbeat.yml&lt;/code> to work after.
As a quick start you can use this one:
&lt;a href="https://github.com/elastic/packetbeat/blob/master/packetbeat.dev.yml">https://github.com/elastic/packetbeat/blob/master/packetbeat.dev.yml&lt;/a> but
probably you would like to change your device from &lt;code>en0&lt;/code> to &lt;code>docker0&lt;/code>.&lt;/p>
&lt;pre>&lt;code>docker build -t agonzalezro/packetbeat .
docker run --net=host agonzalezro/packetbeat
&lt;/code>&lt;/pre>
&lt;p>Cool, now you are ready to go! Everything that you do into your containers will
be show on Kibana, to test it go to you docker host port &lt;code>5601&lt;/code>, it will ask
for an index pattern, change the default &lt;code>logstash-*&lt;/code> to &lt;code>packetbeat-*&lt;/code> and
enjoy it!&lt;/p>
&lt;p>&lt;a href="https://agonzalezro.github.io/packetbeat/kibana.png">&lt;img src="https://agonzalezro.github.io/packetbeat/kibana-thumb.png" alt="kibana-screenshot">&lt;/a>&lt;/p>
&lt;p>If you have any comment, let me know here or in
&lt;a href="http://twitter.com/agonzalezro">twitter&lt;/a>. Also, if you are interested in
monitoring with packetbeat you should definitely take a look to &lt;a href="http://acalustra.com/how-can-i-monitor-my-voip-application.html">&amp;ldquo;Kamailio
monitoring with statsd, SIPCapture or
Packetbeat&amp;rdquo;&lt;/a>
by my friend &lt;a href="http://twitter.com/eloycoto">eloycoto&lt;/a> who introduced me to
packetbeat :a)&lt;/p></description></item><item><title>Dockerizing Pligg with Fig</title><link>https://agonzalezro.github.io/posts/pligg-docker/</link><pubDate>Mon, 24 Nov 2014 00:00:00 +0000</pubDate><guid>https://agonzalezro.github.io/posts/pligg-docker/</guid><description>
&lt;p>For a pet project I needed &lt;a href="http://pligg.com/">Pligg&lt;/a> which is kinda a social
network site in the style of the old-digg (with karma and this kind of things).&lt;/p>
&lt;p>Installing a LAMP system is kinda boring stuff, so, for that and for the sake
of learning I decided to go with &lt;a href="https://www.docker.com/">Docker&lt;/a> &amp;amp;
&lt;a href="http://www.fig.sh/">Fig&lt;/a>. Sadly or luckily, every time that I dockerize
something I find a lot of unexpected problems that slow me down, but&amp;hellip; you
always learn something!&lt;/p>
&lt;p>This post is to explain a little bit the process and the problems that I found
and how I solved them. It&amp;rsquo;s not a how to, it&amp;rsquo;s more a explanation of what I did
and perhaps you can provide a better solution on the comments.&lt;/p>
&lt;h2 id="what-do-i-need">What do I need?&lt;/h2>
&lt;p>The site is &amp;ldquo;simple&amp;rdquo; so the only required stuff will be a MySQL server and an
Apache2 server. You could run 2 Docker instances manually, or&amp;hellip; use the magic
provided by Fig. This is the &lt;code>fig.yml&lt;/code> file that explains my service:&lt;/p>
&lt;pre>&lt;code>web:
build: .
links:
- db
volumes:
- /var/log:/var/www/logs
ports:
- &amp;quot;80:80&amp;quot;
environment:
- MYSQL_PASSWORD
- MY_BASE_URL
db:
image: mysql
volumes:
- /var/lib/mysql:/var/lib/mysql
environment:
- MYSQL_DATABASE=dbpligg
- MYSQL_USER=pligguser
- MYSQL_ROOT_PASSWORD
- MYSQL_PASSWORD
ports:
- &amp;quot;3306:3306&amp;quot;
&lt;/code>&lt;/pre>
&lt;p>Basically I am saying that I will have 2 servers: &amp;ldquo;web&amp;rdquo; &amp;amp; &amp;ldquo;db&amp;rdquo; and specifying
their volumes, environment variables and exported ports.&lt;/p>
&lt;h3 id="problem">Problem?&lt;/h3>
&lt;p>First thing that we see here: &lt;code>MY_BASE_URL&lt;/code> &amp;amp; &lt;code>MYSQL_{,ROOT_}PASSWORD&lt;/code> don&amp;rsquo;t
have any value, this is because Fig is going to got those values from the
current Docker host. I need them for specified some settings file that
originally were written in a normal file (difficult to change while deploying).&lt;/p>
&lt;p>Also, &lt;code>MY_BASE_URL&lt;/code> is a small/ugly trick. It seems that Pligg requires to know
the host were it&amp;rsquo;s running to serve static assets as CSS or JS.&lt;/p>
&lt;p>If you take a look to &lt;a href="https://github.com/agonzalezro/docker-pligg/tree/master/config">my repo &lt;code>config&lt;/code>
folder&lt;/a> you
will see the slightly modified versions of two configuration files for Pligg
that are making use of this environment variables.&lt;/p>
&lt;h3 id="more-problems">More problems?&lt;/h3>
&lt;p>For running Pligg we need a minimal DB structure I&amp;rsquo;ve found 2 different ways of
creating this data in my data container, but none of them are optimal for me,
mainly because they require an extra step:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>I&amp;rsquo;ve created minimal SQL dump with some default values and on the &lt;code>README.md&lt;/code>
provide a quick way of ingest this data using the same container:&lt;/p>
&lt;pre>&lt;code> $ docker exec dockerpligg_db_1 \
mysql -u pligguser -p$MYSQL_PASSWORD dbpligg &amp;lt; &amp;quot;`cat pligg.sql`&amp;quot;
&lt;/code>&lt;/pre>
&lt;/li>
&lt;li>
&lt;p>Another way is going through all the installation process accessing to
&lt;a href="http://example.com/install">http://example.com/install&lt;/a>&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>I think that perhaps creating a script that checks is the DB if empty and if it
is it uses the SQL to dump the DB back would be an option, but seems kinda
dangerous to automate that process in a live environment (anybody said delete
data by a mistaken dump?).&lt;/p>
&lt;p>&lt;a href="http://acalustra.com/">@eloycoto&lt;/a> has recommended me to use inheritance of
containers. But I am not happy with that solution either: I would need two
&lt;code>Dockerfile&lt;/code>s and possibly two &lt;code>fig.yml&lt;/code> files as well or add some weird magic
to replace one container with the other after the installation.&lt;/p>
&lt;h3 id="third-thing-that-i-dont-really-like">Third thing that I don&amp;rsquo;t really like&lt;/h3>
&lt;p>After the first time that you run the installation you need to manually remove
the install path (this seems quite common in PHP apps?). I am doing that
running a &lt;code>docker exec&lt;/code> to that container, but I would prefer to manually
remove it with the &lt;code>Dockerfile&lt;/code>. Why I don&amp;rsquo;t do so? Because if I remove that
folder I am forcing all the users of my configuration to use the dump SQL
method explained above and I don&amp;rsquo;t give them any change.&lt;/p>
&lt;h2 id="so">So&amp;hellip;&lt;/h2>
&lt;p>I suspect that it&amp;rsquo;s normal to have that kind of problems trying to &amp;ldquo;migrate&amp;rdquo; an
application that was never used before in a Docker container.&lt;/p>
&lt;p>To be honest with Pligg, the only changes that I required were minimal, but I
don&amp;rsquo;t know if that &amp;ldquo;install&amp;rdquo; part could be just removed with my own settings
file, I tried that and it was asking me to repeat values that were already set
in the &lt;code>settings.php&lt;/code>.&lt;/p>
&lt;p>I am not happy with the dump/install solution that I&amp;rsquo;ve found, but it works‚Ñ¢!&lt;/p>
&lt;p>I am sure that if you were using Docker or Fig before you will have plenty of
complaints about my article, let me know leaving a comment or just &lt;a href="http://twitter.com/agonzalezro">tweet me
something&lt;/a>.&lt;/p></description></item><item><title>Tailing multiple logs with tmux</title><link>https://agonzalezro.github.io/posts/tailing-with-tmux/</link><pubDate>Sat, 15 Mar 2014 00:00:00 +0000</pubDate><guid>https://agonzalezro.github.io/posts/tailing-with-tmux/</guid><description>
&lt;p>I had a system that was creating a different log for each worker. Those logs
where in the form: &lt;code>/var/log/baselog&lt;/code>, &lt;code>/var/log/baselog.1&lt;/code>,
&lt;code>/var/log/baselog.2&lt;/code> and so on. I wanted to tail them but they were being
updated simultaneously so the output of the tail was being a mess.&lt;/p>
&lt;p>I am a &lt;a href="http://tmux.sourceforge.net/">tmux&lt;/a> user since few months ago (if you
aren&amp;rsquo;t, you should!) so I&amp;rsquo;ve decide to open all of them in different tmux
panes. Thanks to having a pane for every log what we will achieve is the
ability to scroll just one of them, maximize it in case that we need it, move
to a new window, save the buffer&amp;hellip;&lt;/p>
&lt;p>How can you achieve that? Quite simple, just download this script to some
place:&lt;/p>
&lt;!-- raw HTML omitted -->
&lt;p>Once you have saved it installed, run it with the base log file:&lt;/p>
&lt;pre>&lt;code>sh tail_several_logs.sh /var/log/baselog
&lt;/code>&lt;/pre>
&lt;p>The script have some limitations easily fixable:&lt;/p>
&lt;ul>
&lt;li>You should provide a fullpath.&lt;/li>
&lt;li>The logs should be in the form of baselog.N where N is a number.&lt;/li>
&lt;li>I am pretty sure that the loop can be done better.&lt;/li>
&lt;li>And select the layout in every iteration is not the best of the ideas, but it
works.&lt;/li>
&lt;/ul>
&lt;p>Feel free of do whatever you want with the script. I just hope that it&amp;rsquo;s useful
for somebody else.&lt;/p></description></item></channel></rss>